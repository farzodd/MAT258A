{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Toms566\n",
    "using ForwardDiff\n",
    "p = Problem()\n",
    "x0 = p.x0;\n",
    "y0 = p.y0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxed iterations"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000,0.9315609411350718)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newtMin(p, x0, method)\n",
    "    #set up all the variables\n",
    "    k = 0;  #counter\n",
    "    kk = 0; #outer loop counter\n",
    "    f = p.obj(x0);  #f is the objective value at point x\n",
    "    g = p.grd(x0);  #g is the at point x\n",
    "    H = p.hes(x0);  #H is the Hessian at point x\n",
    "    c = p.cons(x0); #c is the constraint at point x\n",
    "    J = ForwardDiff.jacobian(f); #J is the Jacobian of the function\n",
    "    tau = norm(g,2);  #calculate the initial 2-norm of the gradient at x0\n",
    "    rho = 1; #initial value for rho\n",
    "    zeta = 1; #initial value for zeta (our constraint growth checker)\n",
    "    \n",
    "    #We exit after |g| < tauGoal.  Here we scale tauGoal based on the initial value of tau\n",
    "    if tau > 10e5\n",
    "        tauGoal = tau*10e-10\n",
    "    elseif tau > 10e3\n",
    "        tauGoal = tau*10e-4\n",
    "    elseif tau > 10e-3\n",
    "        tauGoal = tau*10e-5\n",
    "    else\n",
    "        tauGoal = tau*10e-4\n",
    "    end\n",
    "    \n",
    "    \n",
    "    if method == \"newton\"\n",
    "        alpha = 1;  #a part of the step size for the newton step\n",
    "        x = x0;  #setting the initial value of x\n",
    "        mu = 10e-4  #this is for the Armijo backtracking linesearch, it helps define a better condition for an appropriate step size\n",
    "        sigma = 10e-5  #this is the minimum value we will accept for an eigenvalue inside the Hessian\n",
    "\n",
    "        #Main loop.  We exit when |g| is less than a predefined value (ie: we are near a stationary point)\n",
    "        while tau > tauGoal\n",
    "            if k == 10e3  #if we max iterations, exit and return the current value\n",
    "                @printf(\"Maxed iterations\")\n",
    "                return (k, p.obj(x))\n",
    "            end\n",
    "\n",
    "            #condition H so that it is definite positive under all conditions\n",
    "            A = eigvals(H)\n",
    "            V = eigvecs(H)\n",
    "            for i in 1:1:length(A)\n",
    "                if abs(A[i]) >= sigma\n",
    "                    A[i] = abs(A[i])\n",
    "                else\n",
    "                    A[i] = sigma\n",
    "                end\n",
    "            end\n",
    "            A = diagm(vec(A))\n",
    "            H = *(*(V,A),transpose(V))\n",
    "\n",
    "            #find search direction\n",
    "            d = H\\-(g)\n",
    "\n",
    "            #Armijo Backtracking\n",
    "            alpha = 1;\n",
    "            while p.obj(x + d*alpha) > (p.obj(x) + alpha*mu*dot(g,d))\n",
    "                alpha = alpha/1.5;\n",
    "            end\n",
    "\n",
    "            #compute next x\n",
    "            x = x + alpha * d;\n",
    "\n",
    "            #clean up for next loop\n",
    "            f = p.obj(x);\n",
    "            g = p.grd(x);\n",
    "            H = p.hes(x);\n",
    "            k = k + 1;\n",
    "            tau = norm(g,2)\n",
    "        end\n",
    "        return (k, p.obj(x))  #return the value we've found\n",
    "    end\n",
    "    \n",
    "    if method == \"augLang\"\n",
    "        tau_c = \n",
    "        tau_g = \n",
    "        \n",
    "        while norm(c) > tau_c && norm(g - *(J',y)) < tau_g\n",
    "            if kk == 10e3  #if we max iterations, exit and return the current value\n",
    "                @printf(\"Maxed iterations\")\n",
    "                return (k, p.obj(x))\n",
    "            end\n",
    "            \n",
    "            L = f - *(y',c) + (rho/2)*(norm(c,2)^2);\n",
    "            gx_L = g - *(J',y-rho*c)\n",
    "            \n",
    "            while norm(gx_L,2) > tauGoal\n",
    "                #condition H so that it is definite positive under all conditions\n",
    "                H_L = ForwardDiff.hessian(L);\n",
    "                g_L = ForwardDiff.gradient(L);\n",
    "                A = eigvals(H_L)\n",
    "                V = eigvecs(H_L)\n",
    "                \n",
    "                for i in 1:1:length(A)\n",
    "                    if abs(A[i]) >= sigma\n",
    "                        A[i] = abs(A[i])\n",
    "                    else\n",
    "                        A[i] = sigma\n",
    "                    end\n",
    "                end\n",
    "                A = diagm(vec(A))\n",
    "                H_L = *(*(V,A),transpose(V))\n",
    "\n",
    "                #find search direction\n",
    "                d = H_L\\-(g_L)\n",
    "\n",
    "                #Armijo Backtracking\n",
    "                alpha = 1;\n",
    "                while p.obj(x + d*alpha) > (p.obj(x) + alpha*mu*dot(g_L,d))\n",
    "                    alpha = alpha/1.5;\n",
    "                end\n",
    "\n",
    "                #compute next x\n",
    "                x = x + alpha * d;\n",
    "\n",
    "                #clean up for next loop\n",
    "                c = p.cons(x);\n",
    "                f = p.obj(x);\n",
    "                J = ForwardDiff.jacobian(f);\n",
    "                L = f - *(y',c) + (rho/2)*(norm(c,2)^2);\n",
    "                H_L = ForwardDiff.hessian(L);\n",
    "                g_L = ForwardDiff.gradient(L);\n",
    "                gx_L = g_L - *(J',y-rho*c)\n",
    "                tau = norm(gx_L,2);\n",
    "                k = k + 1;\n",
    "            end                \n",
    "            \n",
    "            #update y and rho\n",
    "            if norm(c,2) < zeta\n",
    "                rho = rho; #just put here to keep track of algorithm from notes\n",
    "                y = y - rho*c;\n",
    "            else\n",
    "                rho = rho * 10;\n",
    "                y = y;\n",
    "            end\n",
    "            \n",
    "            kk = kk + 1;\n",
    "            c = p.cons(x);\n",
    "            g = p.grd(x);\n",
    "            J = ForwardDiff.jacobian(f);\n",
    "        end\n",
    "        return (k, p.obj(x),p.cons(x))  #return the value we've found, as well as the value of the constraint at that point\n",
    "end\n",
    "\n",
    "#run the minimizer\n",
    "newtMin(p, x0, augLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Nothing to be done\n",
      "INFO: METADATA is out-of-date â€” you may not have the latest version of ForwardDiff\n",
      "INFO: Use `Pkg.update()` to get the latest versions of your packages\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
